<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on PUNYA SWAROOP SIRIGIRI</title>
    <link>https://punyaswaroop12.github.io/post/</link>
    <description>Recent content in Posts on PUNYA SWAROOP SIRIGIRI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 01 Jan 2019 00:00:00 -0600</lastBuildDate>
    
	<atom:link href="https://punyaswaroop12.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ML Algorithms 4:Introduction to Boosting and the famous XGBoost</title>
      <link>https://punyaswaroop12.github.io/post/xgboost-and-lightxgboost/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://punyaswaroop12.github.io/post/xgboost-and-lightxgboost/</guid>
      <description>I am writing these series of posts to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others in an intuitive way
Boosting models are some of the most famous machine learning algorithms. There fame comes from the fact that they learn from their mistakes during the process sequentially. This helps those models to actually build on top of the mistakes and give faster and better results.</description>
    </item>
    
    <item>
      <title>ML Algorithms 3: Bagging and Random forests </title>
      <link>https://punyaswaroop12.github.io/post/bagging-and-random-forests/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://punyaswaroop12.github.io/post/bagging-and-random-forests/</guid>
      <description>I am writing these series of posts to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others in an intuitive way
Bagging Introduction Bagging is an ensemble technique. In this method, instead of training just one tree, we train hundreds of trees and create an ensemble of the output at the end. For each tree, instead of taking the entire data, we take only a few datapoints as a bagging sample.</description>
    </item>
    
    <item>
      <title>ML Algorithms 2: Decision trees</title>
      <link>https://punyaswaroop12.github.io/post/decision-trees/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://punyaswaroop12.github.io/post/decision-trees/</guid>
      <description>I am writing these series of posts to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others in an intuitive way
Introduction Decision trees are a class of machine learning algorithms that decide the output class of a datapoint based on a series of binary decisions using the variables in the dataset. This split is generally called binary recrusive split and happens at each step of the tree.</description>
    </item>
    
    <item>
      <title>ML Algorithms 1:Logistic Regression</title>
      <link>https://punyaswaroop12.github.io/post/logistic-regression/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 -0600</pubDate>
      
      <guid>https://punyaswaroop12.github.io/post/logistic-regression/</guid>
      <description>This is my first post of a series of summaries of machine learning algorithms. I personally took this up to challenge myself to enhance my skill in communciating machine learning and statistical concepts to others. In this post, i would be talking about logistic regression which is still very widely used for classification in multiple industries.
Introduction Logistic regression is primarily a binary classifier which is used when the dependent variable is a categorical variable.</description>
    </item>
    
  </channel>
</rss>